{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "This notebook is to test whether the `neuro_op` module's nodes can successfully infer `world_dist` if only this distribution's information pieces $x_{world}$ are received (i.e., with rates h>0, r=0).\n",
    "\n",
    "For this, the model is run with nodes receiving different amount of information (i.e., different `t_max`, `h`). Then, each node's posterior predictive distribution (*PPD*), equalling its forecast of future incoming information, is obtained via \n",
    "1. sampling of model parameters $\\theta$ proportional to its posterior $p(\\theta | x_{world})$;\n",
    "2. using these sampled model parameters to generate data proportional to the model likelihood $p(x_{PPD}|\\theta_{sampled})$\n",
    "\n",
    "PPDs thereby fully conserves uncertainty by paying respect to both posterior and likelihood stochasticity.\n",
    "\n",
    "We then use the PPDs to quantify the nodes' modelling accuracy by computing the Kullback-Leibler divergence and average MLE distances between node PPDs and data generated by `world_dist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuro_op as nop\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import networkx as nx\n",
    "import numpy as np\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_standard = dict(\n",
    "    N_nodes=100,\n",
    "    N_neighbours=3,\n",
    "    N_beliefs=500,\n",
    "    belief_min=-50,\n",
    "    belief_max=50,\n",
    "    log_priors=np.zeros(500),\n",
    "    likelihood=st.norm(loc=0, scale=5),\n",
    "    world_dist=st.norm(loc=0, scale=5),\n",
    "    h=1,\n",
    "    r=1,\n",
    "    t0=0,\n",
    "    t_max=10000,\n",
    "    t_sample=250,\n",
    "    sample_bins=50,\n",
    "    sample_opinion_range=[-20, 20],\n",
    "    sample_p_distance_params=[[1, 1], [2, 1]],\n",
    ")\n",
    "\n",
    "output_variables = [\n",
    "    \"nodes\",\n",
    "    \"G\",\n",
    "    \"beliefs\",\n",
    "    \"world\",\n",
    "    \"N_events\",\n",
    "    \"t_end\",\n",
    "    \"mu_nodes\",\n",
    "    \"kl_divs\",\n",
    "    \"p_distances\",\n",
    "    \"RANDOM_SEED\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 1, 2\n",
    "\n",
    "Varying parameters:\n",
    "- $t_max = 1eX, X\\in\\{2,3,4,5,6,7^?\\}$\n",
    "- $N_{agents} = 1eX, X\\in\\{1,2,3,4,5\\}$\n",
    "\n",
    "Figure 1:\n",
    "- histogram $\\mu_i$ (for now MLEs)\n",
    "\n",
    "Figure 2:\n",
    "- $3d^?$ plot w. $(x,y,z) = (t_{max}, N_{agents}, t_{sim})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No. events = (e.g.) 20000\n",
    "    - for N=100, r+h=2, t=10000\n",
    "-> No. events = (r+h)/100 * N * t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t =  2 \t N =  1\n",
      "Sampling at t= 0\n",
      "t =  2 \t N =  2\n",
      "Sampling at t= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jens/Documents/Repos/neuro_op/src/neuro_op/neuro_op.py:223: RuntimeWarning: invalid value encountered in divide\n",
      "  ppd_world_out = ppd_world_out[0] / np.sum(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jens/Documents/Repos/neuro_op/src/neuro_op/neuro_op.py:135: RuntimeWarning: divide by zero encountered in log\n",
      "  terms = P * np.log(P / Q)\n",
      "/home/jens/Documents/Repos/neuro_op/src/neuro_op/neuro_op.py:135: RuntimeWarning: invalid value encountered in multiply\n",
      "  terms = P * np.log(P / Q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t =  2 \t N =  3\n",
      "Sampling at t= 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28minput\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN_nodes\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mN\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt = \u001b[39m\u001b[38;5;124m\"\u001b[39m, t, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m N = \u001b[39m\u001b[38;5;124m\"\u001b[39m, N)\n\u001b[1;32m      9\u001b[0m     tmp\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(output_variables,\n\u001b[0;32m---> 11\u001b[0m                  \u001b[43mnop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     12\u001b[0m     )\n\u001b[1;32m     13\u001b[0m out\u001b[38;5;241m.\u001b[39mappend(tmp)\n",
      "File \u001b[0;32m~/Documents/Repos/neuro_op/src/neuro_op/neuro_op.py:424\u001b[0m, in \u001b[0;36mrun_model\u001b[0;34m(N_nodes, N_neighbours, N_beliefs, belief_min, belief_max, log_priors, likelihood, world_dist, h, r, t0, t_max, t_sample, sample_bins, sample_opinion_range, sample_p_distance_params)\u001b[0m\n\u001b[1;32m    421\u001b[0m G = build_random_network(N_nodes, N_neighbours)\n\u001b[1;32m    422\u001b[0m world = node(beliefs=beliefs, log_priors=world_dist.logpdf(x=beliefs))\n\u001b[0;32m--> 424\u001b[0m # Renormalize rates to keep rate per node constant (division by 100 to keep input's order of magnitude around 1)\n\u001b[1;32m    425\u001b[0m h = h * N_nodes / 100\n\u001b[1;32m    426\u001b[0m r = r * N_nodes / 100\n",
      "File \u001b[0;32m~/Documents/Repos/neuro_op/src/neuro_op/neuro_op.py:339\u001b[0m, in \u001b[0;36mnetwork_dynamics\u001b[0;34m(nodes, G, world, h, r, t0, t_max, t_sample, sample_bins, sample_opinion_range, sample_p_distance_params)\u001b[0m\n\u001b[1;32m    336\u001b[0m else:\n\u001b[1;32m    337\u001b[0m     # edge event\n\u001b[1;32m    338\u001b[0m     chatters = random.choice(list(G.edges()))\n\u001b[0;32m--> 339\u001b[0m     # update each node's log-probabilities with sample of edge neighbour's beliefs\n\u001b[1;32m    340\u001b[0m     sample0 = nodes[chatters[0]].get_belief_sample(size=1)\n\u001b[1;32m    341\u001b[0m     sample1 = nodes[chatters[1]].get_belief_sample(size=1)\n",
      "File \u001b[0;32m~/Documents/Repos/neuro_op/src/neuro_op/neuro_op.py:48\u001b[0m, in \u001b[0;36mnode.get_belief_sample\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Sample a belief according to world model.\"\"\"\u001b[39;00m\n\u001b[1;32m     47\u001b[0m probs \u001b[38;5;241m=\u001b[39m logpdf_to_pdf(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_probs)\n\u001b[0;32m---> 48\u001b[0m sample \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeliefs, size\u001b[38;5;241m=\u001b[39msize, p\u001b[38;5;241m=\u001b[39mprobs)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiary_out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiary_out, sample)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sample\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for t in [2, 3, 4, 5, 6]:\n",
    "    input = input_standard.copy()\n",
    "    input[\"t_max\"] = 10**t\n",
    "    tmp = []\n",
    "    for N in [2, 3, 4, 5]:\n",
    "        input[\"N_nodes\"] = 10**N\n",
    "        print(\"t = \", t, \"\\t N = \", N)\n",
    "        tmp.append(dict(zip(output_variables, nop.run_model(**input))))\n",
    "    out.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "for t in [2, 3]:\n",
    "    print(10**t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (neuro_op)",
   "language": "python",
   "name": "neuro_op"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

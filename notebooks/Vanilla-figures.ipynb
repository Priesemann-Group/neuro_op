{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "This notebook is to test whether the `neuro_op` module's nodes can successfully infer `world_dist` if only this distribution's information pieces $x_{world}$ are received (i.e., with rates h>0, r=0).\n",
    "\n",
    "For this, the model is run with nodes receiving different amount of information (i.e., different `t_max`, `h`). Then, each node's posterior predictive distribution (*PPD*), equalling its forecast of future incoming information, is obtained via \n",
    "1. sampling of model parameters $\\theta$ proportional to its posterior $p(\\theta | x_{world})$;\n",
    "2. using these sampled model parameters to generate data proportional to the model likelihood $p(x_{PPD}|\\theta_{sampled})$\n",
    "\n",
    "PPDs thereby fully conserves uncertainty by paying respect to both posterior and likelihood stochasticity.\n",
    "\n",
    "We then use the PPDs to quantify the nodes' modelling accuracy by computing the Kullback-Leibler divergence and average MLE distances between node PPDs and data generated by `world_dist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy  # deep-copying of input dictionary (which includes mutable objects)\n",
    "import gc  # explicit garbace collection calling after each run\n",
    "import neuro_op as nop  # project's main module\n",
    "import matplotlib.pyplot as plt  # because Figures >> Text\n",
    "\n",
    "# import networkx as nx          # network initialization\n",
    "import numpy as np  # ...of course we need numpy\n",
    "import pickle  # output export/import\n",
    "import scipy.stats as st  # ...of course we need scipy\n",
    "import time  # runtime measuring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 1, 2\n",
    "\n",
    "Varying parameters:\n",
    "- $t_{max} = 1eX, X\\in\\{2,3,4,5,6,7^?\\}$\n",
    "- $N_{agents} = 1eX, X\\in\\{1,2,3,4,5\\}$\n",
    "\n",
    "Figure 1:\n",
    "- histogram $\\mu_i$ (for now MLEs)\n",
    "\n",
    "Figure 2:\n",
    "- $3d^?$ plot w. $(x,y,z) = (t_{max}, N_{agents}, t_{sim})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current adaptions:\t dict_items([('N_nodes', 10)])\n",
      "For adaptions\n",
      " dict_items([('N_nodes', 10)])  :\n",
      "t_exec =  0.6124289035797119\n"
     ]
    }
   ],
   "source": [
    "def model_scan(dict_list):\n",
    "    \"\"\"\n",
    "    Call 'run_model' with 'input_standard' adapted at specified dictionary entries.\n",
    "\n",
    "    Serially run multiple model parameter sets, safe output to pickle file, garbage collect memory.\n",
    "    \"\"\"\n",
    "\n",
    "    for dic_tmp in dict_list:\n",
    "        input = copy.deepcopy(nop.input_standard)\n",
    "        for key, value in dic_tmp.items():\n",
    "            input[key] = value\n",
    "        print(\"Current adaptions:\\t\", dic_tmp.items())\n",
    "        t0 = time.time()\n",
    "        output = dict(nop.run_model(**input))\n",
    "        t1 = time.time()\n",
    "        output[\"t_exec\"] = t1 - t0\n",
    "        print(\"For adaptions\\t\", dic_tmp.items(), \" :\\n\\t t_exec = \", (t1 - t0))\n",
    "        filename = (\n",
    "            \"out\"\n",
    "            + str(dic_tmp)\n",
    "            + time.strftime(\"--%Y-%m-%d--%H-%M--\", time.localtime(t0))\n",
    "            + \"--export.pkl\"\n",
    "        )\n",
    "        with open(filename, \"wb\") as f:\n",
    "            pickle.dump(output, f)\n",
    "        del output\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'N_nodes': 10, 't_max': 10},\n",
       " {'N_nodes': 10, 't_max': 100},\n",
       " {'N_nodes': 10, 't_max': 1000},\n",
       " {'N_nodes': 10, 't_max': 10000},\n",
       " {'N_nodes': 100, 't_max': 10},\n",
       " {'N_nodes': 100, 't_max': 100},\n",
       " {'N_nodes': 100, 't_max': 1000},\n",
       " {'N_nodes': 100, 't_max': 10000},\n",
       " {'N_nodes': 1000, 't_max': 10},\n",
       " {'N_nodes': 1000, 't_max': 100},\n",
       " {'N_nodes': 1000, 't_max': 1000},\n",
       " {'N_nodes': 1000, 't_max': 10000},\n",
       " {'N_nodes': 10000, 't_max': 10},\n",
       " {'N_nodes': 10000, 't_max': 100},\n",
       " {'N_nodes': 10000, 't_max': 1000},\n",
       " {'N_nodes': 10000, 't_max': 10000},\n",
       " {'t_max': 100000},\n",
       " {'t_max': 1000000},\n",
       " {'N_nodes': 10, 'r': 0.1, 'h': 5},\n",
       " {'N_nodes': 100, 'r': 0.1, 'h': 5},\n",
       " {'N_nodes': 1000, 'r': 0.1, 'h': 5},\n",
       " {'N_nodes': 10000, 'r': 0.1, 'h': 5},\n",
       " {'N_nodes': 10, 'r': 5, 'h': 0.1},\n",
       " {'N_nodes': 100, 'r': 5, 'h': 0.1},\n",
       " {'N_nodes': 1000, 'r': 5, 'h': 0.1},\n",
       " {'N_nodes': 10000, 'r': 5, 'h': 0.1},\n",
       " {'N_nodes': 10, 'r': 5, 'h': 5},\n",
       " {'N_nodes': 100, 'r': 5, 'h': 5},\n",
       " {'N_nodes': 1000, 'r': 5, 'h': 5},\n",
       " {'N_nodes': 10000, 'r': 5, 'h': 5},\n",
       " {'N_nodes': 100000}]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = []\n",
    "for N in [1, 2, 3, 4]:\n",
    "    for t in [1, 2, 3, 4]:\n",
    "        params.append(dict(N_nodes=10**N, t_max=10**t))\n",
    "\n",
    "for t in [5, 6]:\n",
    "    params.append(dict(t_max=10**t))\n",
    "\n",
    "for r, h in [(0.1, 5), (5, 0.1), (5, 5)]:\n",
    "    for N in [1, 2, 3, 4]:\n",
    "        params.append(dict(N_nodes=10**N, r=r, h=h))\n",
    "\n",
    "for N in [5]:\n",
    "    params.append(dict(N_nodes=10**N))\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scan(params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuro_op",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
